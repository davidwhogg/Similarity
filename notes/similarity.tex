\documentclass[12pt,letterpaper]{article}

% typography
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\equationname}{equation}

% math
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\renewcommand{\det}[1]{||{#1}||}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}{\mathcal{N}}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{\raggedright How similar are two noisily observed vectors?}

\noindent
\textbf{David W. Hogg}\\
\textsl{CCPP, NYU Physics \\
        Flatiron Institute \\
        MPIA}

\bigskip

\paragraph{Abstract:}
I present a general (although assumption-laden) method for asking the
question: Are two independently and noisily observed objects in fact
identical? My approach is to turn this question into a parameter
estimation, estimating the (latent) difference between the objects,
and evaluating the posterior density at vanishing difference. The
method accounts both for the fact that the (latent, never
instantiated) noiseless vectors are drawn from a compact pdf, and that
there are differences in the vectors that might not be considered
significant, either because of measurement noise, or because of
intrinsic differences that somehow ``don't matter''. The approach
makes heavy use of Gaussians for their mathematical simplicity. In a
sensible limit, the answer reduces to pure chi-squared difference
between the observations. I discuss the results in the context of
stellar spectroscopy (stellar twins) and chemical-abundance-space
measurements of stars (chemical tagging).

\paragraph{Introduction:}
Imagine that $y_1$ and $y_2$ are two observed data points. Think of
them as $D$-dimensional vectors in a vector space. They could be two
spectra of two stars (Hawkins's problem), or they could be the
detailed element abundances for two stars (Ness's problem). How can we
tell if the two objects observed are intrinsically identical? Would
they appear identical if we had better data?

One option would be to think of making two hypotheses, to wit, ``H1:
The two objects are identical'', and ``H2: The two objects are
different''. Then we could compute the marginalized likelihoods of
these two hypotheses under some priors over the nuisance parameters of
relevance. That's sensible (and more-or-less what we did in S. M. Oh's
paper).

Another option would be to convert this problem into one of parameter
estimation: That is, compute a marginalized likelihood or marginalized
posterior probability density (pdf) for the \emph{difference} between
the two objects, and see if that likelihood or posterior pdf puts
significant density on zero---on a null or vanishing difference
vector.

For various reasons that are (perhaps) badly justified, we are going
to take the second approach in this \documentname. That is, we are
going to estimate the difference between the objects given their
(presumed noisy) measurements, and find out if that difference is
consistent with zero, or with the kind of difference that we would count
as effectively zero (more on this below).

Before we start, one side note: I usually recommend that one just
compute chi-squared between the vectors. This is defined\footnote{In
  \equationname~(\ref{eq:chisquared}), I provide a linear-algebra
  version of the chi-squared formula, which is usually given as a sum
  over independent data points, like
  \[\chi^2\equiv\sum_d\frac{[y_{1d}-y_{2d}]^2}{\sigma_{1d}^2+\sigma_{2d}^2} \quad .\]
  The linear-algebra expression I give in (\ref{eq:chisquared}) is
  identical to this classic sum in the case that the covariance matrix
  $C_1 + C_2$ is diagonal, with variances $\sigma_{1d}^2+\sigma_{2d}^2$ on the
  diagonal.} as
\begin{eqnarray}
\chi^2 &\equiv&
\transpose{[y_1 - y_2]}\cdot\inverse{[C_1 + C_2]}\cdot [y_1 - y_2] \quad ,
\label{eq:chisquared}
\end{eqnarray}
where, implicitly, $y_1$ and $y_2$ are $D$-dimensional column vectors,
and $C_1$ and $C_2$ are the $D\times D$ covariance matrices that
express our beliefs about the noise variance on the measurements $y_1$
and $y_2$. In this case (that is, if you just compute $\chi^2$) we say
that the two objects are identical if the $\chi^2$ is below some
threshold. That is simple, easy, and probabilistically
justifiable. The objections are manifold. One is that, as the data get
better (the determinants of the $C_1$ and $C_2$ matrices get smaller),
it becomes increasingly harder to conclude similarity or
identicality. To this I say: \emph{Duh!} That will be true of any
correct method. Another objection is that $\chi^2$ treats all elements
of the data (all directions in the vector space) identically, or at
least weights them by their measurement quality rather than their
\emph{usefulness for determining dissimilarity}. If some dimensions
are better than others for determining identicality, $\chi^2$ is blind
to that. That is a significant objection, and the one I
address---below!---in this \documentname.

\paragraph{Assumptions:}
I am going to make the following assumptions, each one of which is
questionable and unpleasant, but each one of which improves our
computational tractability.
\begin{enumerate}\itemsep0ex
\item \emph{Noise:} Observed vectors $y_1$ and $y_2$ are noisy
  measurements of some latent, unobserved \emph{true} vectors $x_1$
  and $x_2$, and the differences between the observed and true vectors
  are drawn from a $D$-dimensional Gaussian distribution with zero
  mean and known variance tensors $C_1$ and $C_2$, respectively. The
  noise draws are independent (but not identically distributed). There
  are no outliers or non-Gaussianities, and the variance tensors are
  not just known but correct. These covariance matrices can contain
  infinite eigenvalues\footnote{Something about what an eigenvalue is,
    and in the special case of the diagonal matrix.} (that is,
  components or eigen-directions about which there is no information),
  but they probably shouldn't contain zero eigenvalues (that is,
  directions about which there is perfect knowledge).
\item \emph{Prior:} The true vectors are drawn independently from a
  Gaussian distribution of mean $\mu$ and variance tensor $\frac{1}{2}\,V$. This
  variance tensor might have have some zero eigenvalues (that is, it
  might be low-rank), or it might have some infinite eigenvalues; we
  will write down equations that are safe to these possibilities. By
  some magic or algorithm or intuition, the mean vector $\mu$ and
  variance tensor $\frac{1}{2}\,V$ are known in advance.
\item \emph{Intrinsic scatter:} There are certain kinds of differences
  between vectors that are unimportant to us. That is, if they are
  different in these uninteresting ways, we don't consider them to be
  different. We will assume that these intrinsic differences are also
  drawn from a Gaussian, but this time with zero mean, and variance
  tensor $W$, again known in advance. This variance tensor $W$ better
  be compact---in some sense---relative to the prior variance $V$, or
  else all the vectors will be considered similar! Interestingly, the
  variance $W$ can be made compact in the relevant ways by having
  small eigenvectors, or by being low-rank, or both. Even more
  interestingly, it can even have very large (infinite, even)
  eigenvectors provided that it is low rank. More on this below.
\end{enumerate}

\end{document} % syntactical reference point

The first thing we are going to do is diagonalize the prior variance
tensor $V$. We are going to do this so we develop an approach that is
computationally feasible when $V$ is low-rank, and numerically stable
when $V$ has a bad condition number. We will write
\begin{eqnarray}
V &\equiv& R\cdot\Lambda\cdot\transpose{R} \quad ,
\end{eqnarray}
where $R$ is a $D\times K$ matrix of the $K$ ($K\leq D$) eigenvectors
of $V$ that have positive eigenvalues, and $\Lambda$ is the diagonal
$K\times K$ tensor that has the eigenvalues on its diagonal. By the
assumption that the true vectors are drawn from the Gaussian with this
variance and mean $\mu$, we can (without any loss of generality) write
the \emph{true} vectors $x_1$ and $x_2$ as a sum and difference of
coefficient vectors $a$ and $b$ in the $K$-dimensional space as
follows:
\begin{eqnarray}
x_1 &=& R\cdot[a + b] + \mu \\
x_2 &=& R\cdot[a - b] + \mu \quad ,
\end{eqnarray}
where the $D$-vector $R\cdot a + \mu$ is the mean of $x_1$ and $x_2$,
and $R\cdot b$ is the half-difference between $x_1$ and $x_2$. This is
a change of variables.  In this change of variables, the question of
``are $y_1$ and $y_2$ identical?'' will convert to the question of
``is the posterior pdf for $b$ consistent with $b=0$?''. We need a
posterior pdf for $b$!

\paragraph{Bayesian inference:}
Under these assumptions (above), there is only one thing to do:
Compute the posterior pdf for $b$, marginalizing out $a$, given the
data $y_1$ and $y_2$, and our assumptions (which are detailed, and
include the objects $C_1$, $C_2$, $\mu$, and $V$).
\begin{eqnarray}
p(b\given y_1,y_2) &=& \int p(a,b\given y_1,y_2)\,\dd a \\
p(a,b\given y_1,y_2) &=& \frac{1}{Z}\,p(a,b)\,p(y_1\given a,b)\,p(y_2\given a,b) \\
p(a,b) &=& \normal(a+b\given 0,\Lambda)\,\normal(a-b\given 0,\Lambda) \\
p(y_1\given a,b) &=& \normal(y_1\given R\cdot[a+b]+\mu,C_1) \\
p(y_2\given a,b) &=& \normal(y_2\given R\cdot[a-b]+\mu,C_2), \quad
\end{eqnarray}
where the integral is over all values of $a$, the normalization
constant $Z$ can be ignored for now, and the function $\normal(x\given
m,v)$ is the Gaussian pdf for $x$ given mean $m$ and variance
$v$. Switching to the log (and making use of the magic of Gaussians),
\begin{eqnarray}
\ln p(a,b\given y_1,y_2) &=&
  -\frac{1}{2}\,\transpose{[a+b]}\cdot\inverse{\Lambda}\cdot [a+b]
  -\frac{1}{2}\,\transpose{[a-b]}\cdot\inverse{\Lambda}\cdot [a-b] \nonumber\\
  &&
  -\frac{1}{2}\,\transpose{[y_1-R\cdot(a+b)-\mu]}\cdot\inverse{C_1}\cdot [y_1-R\cdot(a+b)-\mu] \nonumber\\
  &&
  -\frac{1}{2}\,\transpose{[y_2-R\cdot(a-b)-\mu]}\cdot\inverse{C_2}\cdot [y_2-R\cdot(a-b)-\mu] \nonumber\\
  &&
  + Z \quad ,
\end{eqnarray}
where $Z$ is a constant that doesn't matter to our inference. All we
have to do now is complete the square for $a$, exponentiate, do the
$a$ integral, and look at the $b$-dependence of the result.
This result is also quadratic in $b$, so really we complete the square
in $a$ and then take the constant term, and complete the square for
\emph{that} in $b$.

When we do the double completion of squares, we get an absolute
hellacious mess\footnote{Yes, I really do have this worked out in my
  orange notebook, under the date 2017-03-13, with only a few
  uncertain signs.}.
One reaction to this mess is to consider the
unrealistic but comforting case of $C_1=C_2\equiv C$; that is, the case in
which both objects are observed with very similar measurement noise.
In this case, the $a$-marginalized posterior becomes
\begin{eqnarray}
\ln p(b\given y_1,y_2) &=&
  -\frac{1}{2}\,\transpose{[b-\beta]}\cdot\inverse{W}\cdot [b-\beta]
  -\frac{1}{2}\,\ln\det{2\pi\,W} \\
\inverse{W} &\equiv&
  2\,\inverse{\Lambda} + 2\,\transpose{R}\cdot\inverse{C}\cdot R \\
\beta &\equiv&
  W\cdot\transpose{R}\cdot\inverse{C}\cdot [y_1 - y_2]
\quad ,
\end{eqnarray}
where the log-det term is a normalization constant that ensures a unit
integral in the $b$ space. Note that this solution has the very nice
property that if $R$ is not square---that is, the prior variance is
low rank---all computations are done in the lower-dimension basis,
with the operator $W\cdot\transpose{R}\cdot\inverse{C}$ projecting the
data difference $[y_1-y_2]$ to the (possibly) lower-dimension basis
where the intrinsic variance $V$ is diagonal.

In terms of generating a scalar: The option we opened with, above, is
to evaluate this log probability $\ln p(b\given y_1,y_2)$ at zero
separation $b=0$. That is, to use the scalar $Q$:
\begin{eqnarray}
Q &\equiv&
  -\frac{1}{2}\,\transpose{\beta}\cdot\inverse{W}\cdot\beta
  -\frac{1}{2}\,\ln\det{2\pi\,W} \quad ,
\end{eqnarray}
with $\beta$ and $W$ defined above in terms of the data and variances.

This scalar $Q$ has lots of good properties, including that of being
probabilistically justified, and that of not depending incredibly
strongly on the prior variance $\Lambda$ (especially when the data are
high in signal-to-noise). Another good (and surprising) property is
that it becomes exactly $-\chi^2/2$ (plus a constant), where $\chi^2$
is defined above in \equationname~(\ref{eq:chisquared}), precisely when the
measurements are better than the prior variance along all directions,
and the prior variance is full-rank (so $\transpose{R}\cdot R\equiv R\cdot\transpose{R}$ is the
identity).

So we weren't so wrong to be using $\chi^2$ in the first place.

\end{document}
