% This project is part of the Similarity and Data Analysis Recipes projects.
% Copyright 2017 the author.

% # To-Do:
% - deal with HOGG CITE etc..
% - re-check that abstract tells the whole story.
% - need to cite out to or refer to Sam's linear algebra and Gaussian notes.
% - idea that you could get a low-rank W out of a regression...?
% - get comments from Ness, Bedell, Rix, Hawkins.
% - ask Leistedt to re-derive Q expression.

\documentclass[12pt,letterpaper]{article}
\usepackage[hidelinks]{hyperref}

% typography
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\equationname}{equation}
\newcommand{\etal}{\foreign{et al.}}
\linespread{1.08}
\setlength{\parindent}{1.1\baselineskip}
\sloppy\sloppypar\raggedbottom\frenchspacing

% math
\newcommand{\tra}[1]{{#1}^{\mathsf{T}}}
\newcommand{\inv}[1]{{#1}^{-1}}
\renewcommand{\det}[1]{||{#1}||}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}{\mathcal{N}}

\begin{document}

\section*{\raggedright Data Analysis Recipes:
  How similar are two noisily observed vectors?%
\footnote{This \documentname\ is copyright 2017 the author. Feel free to copy and
distribute it, provided that you make no changes to it whatsoever.}}

\noindent
\textbf{David W. Hogg%
\footnote{For many of the ideas and some of the presentation in this \documentname, I
am indebted to Dan Foreman-Mackey (UW) and Hans-Walter Rix (MPIA).}}\\
\textsl{\footnotesize
  Center for Cosmology and Particle Physics, Department of Physics, New York University \\
  Center for Data Science, New York University \\
  Flatiron Institute, a division of the Simons Foundation \\
  Max-Planck-Institut f\"ur Astronomie, Heidelberg}

\paragraph{Abstract:}
I present a general (although assumption-laden) method for asking the
question: Are two independently and noisily observed objects in fact
identical? My approach is to turn this question into a parameter
estimation, estimating the (latent) difference between the objects,
and evaluating the posterior density at vanishing difference. The
method accounts both for the fact that the (latent, never
instantiated) noiseless vectors are drawn from a compact pdf, and for the fact that
there are differences in the vectors that might not be considered
significant, either because of measurement noise, or because of
intrinsic differences that somehow ``don't matter''. The approach
makes heavy use of Gaussians for their mathematical simplicity. In a
sensible limit, the answer reduces to pure chi-squared difference
between the observations. I discuss the method and results in the context of some
astrophysics problems---stellar spectroscopy (stellar twins) and
chemical-abundance-space measurements (chemical tagging)---but everything here
should be accessible to any natural scientist.

\paragraph{Introduction:}
The subjective motivation for writing this \documentname\ is the odd coincidence
that during this academic year\footnote{2016--2017}, three different groups
have brought to me questions about \emph{identicality}: How do you
determine that two objects, both of which have been noisily observed,
are in fact identical, in respects that we care about? One collaboration
(Oh \etal, 2017)
wanted to find pairs of stars, noisily observed by
the \project{Gaia} Mission (\project{Gaia} Collaboration, 2016), that have identical spatial
velocities.
Another collaboration (Ness \etal, 2017) wanted to find pairs
of stars, noisily observed by the \project{\acronym{APOGEE}}
spectrograph (Majewski \etal, 2015), that have identical chemical abundances, despite the fact
that (as a result of their different surface temperatures and surface
gravities) they don't in fact look similar.
Yet another (Hawkins \etal, in progress) has wanted to find stars that
look identical in every way, spectrally---stars that are perfect stunt
doubles---using the spectrographic data (the spectrograms, as it were)
directly. In all three of these cases, I gave advice, but in all three
cases, I became slightly unhappy both with the advice I was giving, and
also with the availability of extremely clear expositions on this subject
in the libraries of astrophysicists.\footnote{But I must admit that I didn't
  look very hard.}
It is my hope that this \documentname\ will be useful to researchers working
outside---even far outside---the domain of astrophysics. These ideas and methods 
are completely general.

Imagine that $y_1$ and $y_2$ are two observed data points. Think of
them as $D$-dimensional vectors in a vector space. They could be the
pixel values in the observed spectra of two stars (in which case $D$
is the number of pixels in the observed spectrum) or they could be
the detailed element abundances measured for two stars (in which case
$D$ is the number of measured elements). How can we tell if the two
objects observed are intrinsically identical? Would they appear
identical if we had better data?\footnote{I found myself saying,
  recently, that in an observational science (like astronomy), you
  have to define the words ``true'' and ``truly'' so that they can be
  operationally satisfied (and we did so, in Foreman-Mackey \etal, 2014).
  Are two stars \emph{truly} identical?  I
  find that I want always to convert such questions to
  counter-factuals about far better data: Will the two stars' data
  become more and more similar as we take better and better data?}

One option would be to think of making two hypotheses, to wit, ``H1:
The two objects are identical'', and ``H2: The two objects are
different''. Then we could compute the marginalized likelihoods of
these two hypotheses under some priors over the nuisance parameters of
relevance. That's sensible (and more-or-less what we did in Oh \etal, 2017).

Another option would be to convert this problem into one of parameter
estimation: That is, compute a marginalized likelihood or marginalized
posterior probability density (pdf) for the \emph{difference} between
the two objects, and see if that likelihood or posterior pdf puts
significant density on zero---on a null or vanishing difference
vector.

For various reasons that are (perhaps) badly justified\footnote{%
  It is my experience that hypothesis testing, or model comparison, is
  much harder than parameter estimation. This is not always true, but
  often, since model comparison involves performing full integrals of
  a type that cannot be performed with Markov Chain Monte Carlo (not that
  this consideration is directly relevant here),
  while parameter estimation can proceed with smaller integrals, and of
  the type that \emph{can} be performed with \acronym{MCMC}. Now a model
  comparison can only be converted into a parameter estimation when the
  models are somehow related or nested, as they are in this case. That is,
  the case of two vectors being identical is the vanishing-separation limit
  of the case of two vectors being different. I also have various bits of
  philosophy about prior-dependence that are more or less relevant, depending
  on context. And don't get me started on long-term future-discounted free
  cash flow (which is relevant, believe it or not). I have removed all of this
  from the main text because I don't want the reader to
  get bogged down with philosophy. I left it in this footnote, however, to
  suggest to the reader that there is \emph{some} thinking behind the opaque
  choices made here and in what follows.}
we are going
to take the second approach in this \documentname. That is, we are
going to estimate the true (vector) difference between the objects, given their
(noisy) measurements, and find out if that difference is
consistent with zero, or with the kind of difference that we would count
as effectively zero (more on this below).

Before we start, one side note: I usually recommend that one just
compute chi-squared between the vectors. This is defined\footnote{In
  \equationname~(\ref{eq:chisquared}), I provide a linear-algebra
  version of the chi-squared formula, which is usually given as a sum
  over independent data points, like
  \[\chi^2\equiv\sum_d\frac{[y_{1d}-y_{2d}]^2}{\sigma_{1d}^2+\sigma_{2d}^2} \quad .\]
  The linear-algebra expression I give in (\ref{eq:chisquared}) is
  identical to this classic sum in the case that the covariance matrix
  $C_1 + C_2$ is diagonal, with variances $\sigma_{1d}^2+\sigma_{2d}^2$ on the
  diagonal.} as
\begin{eqnarray}
\chi^2 &\equiv&
\tra{[y_1 - y_2]}\cdot\inv{[C_1 + C_2]}\cdot [y_1 - y_2] \quad ,
\label{eq:chisquared}
\end{eqnarray}
where, implicitly, $y_1$ and $y_2$ are $D$-dimensional column vectors,
and $C_1$ and $C_2$ are the $D\times D$ covariance matrices that
express our beliefs about the noise variance on the measurements $y_1$
and $y_2$. In this case (that is, if you just compute $\chi^2$) we say
that the two objects are identical if the $\chi^2$ is below some
threshold. That is simple, easy, and probabilistically
justifiable. The objections are manifold. One is that, as the data get
better (the determinants of the $C_1$ and $C_2$ matrices get smaller),
it becomes increasingly harder to conclude similarity or
identicality. To this I say: \emph{Duh!} That will be true of any
correct method.\footnote{%
  As some statisticians like to say: Chi-squared is just a measure of
  the size of your data! That is, as you get more or better data,
  chi-squared will generally increase and (in this case) the vectors will
  tend to diverge.}
Another objection is that $\chi^2$ treats all elements
of the data (all directions in the vector space) identically, or at
least weights them by their measurement quality rather than their
\emph{usefulness for determining dissimilarity}. If some dimensions
are better than others for determining identicality, $\chi^2$ is blind
to that. That is a significant objection, and the one I
address---below!---in this \documentname.

One final comment: This \documentname\ does not contain,
in any sense or by any implication, any original idea or method in the
domain of statistics, applied mathematics, or astrophysics. It is
merely a pedagogical document, designed to clarify and to entertain.
Please approach it with that expectation.

\paragraph{Assumptions:}
I am going to make the following assumptions%
\footnote{It is my view that a data-analysis method or code
  can be said to be \emph{correct} or (heaven forbid)
  ``\emph{optimal}'' (yes, I am using \emph{scare quotes}) only if there
  is a sufficiently clear set of (explicitly stated) assumptions such
  that the method or code can be shown to be correct or optimal \emph{under
  those explicit assumptions}. That is, we can't talk about doing
  something correctly if we can't set out clearly the rules are under which
  correctness can be defined. Therefore the assumptions laid out here
  represent \emph{the main point} of this \documentname.},
each one of which is
questionable and unpleasant, but each one of which improves our
computational tractability.
\begin{enumerate}\itemsep0ex
\item \emph{Noise:} Observed vectors $y_1$ and $y_2$ are noisy
  measurements of some latent, unobserved \emph{true} vectors $x_1$
  and $x_2$, and the differences between the observed and true vectors
  are drawn from a $D$-dimensional Gaussian distribution with zero
  mean and known variance tensors $C_1$ and $C_2$, respectively. The
  noise draws are independent (but not identically distributed). There
  are no outliers or non-Gaussianities, and the variance tensors are
  not just known but \emph{correctly known}. These covariance matrices can contain
  infinite eigenvalues\footnote{%
    If you don't remember what an eigenvalue or an eigenvector is,
    consult \textit{Wikipedia} or any notes on linear algebra. In the
    case of a perfectly diagonal matrix (all off-diagonal elements
    vanish), the eigenvalues are the diagonal elements.}
  (that is,
  components or eigen-directions about which there is no information),
  but they probably shouldn't contain zero eigenvalues (that is,
  directions about which there is perfect knowledge or along which there is no noise).
\item \emph{Prior:} The true vectors are drawn independently from a
  Gaussian distribution of mean $\mu$ and variance tensor\footnote{%
    I use half-$V$ here---and not $V$---to simplify expressions that follow. Trust me!}
  $\frac{1}{2}\,V$. This variance tensor might have have some zero
  eigenvalues (that is, it might be low-rank), or it might have some
  infinite eigenvalues; we will write down equations that are safe to
  these possibilities. By some magic or algorithm\footnote{I have
    developed two relevant algorithms here. One is called Extreme
    Deconvolution (Bovy \etal, 2011), and one is called Heteroskedastic
    Matrix Factorization (Tsalmantza \& Hogg, 2012).} or intuition, the mean vector
  $\mu$ and variance tensor $\frac{1}{2}\,V$ are known in advance.
\item \emph{Intrinsic scatter:} There might be certain kinds of differences
  between vectors that are unimportant to us. That is, if they are
  different in these uninteresting ways, we don't consider them to be
  different. We will assume that these intrinsic differences are also
  drawn from a Gaussian, but this time with zero mean, and variance
  tensor $W$, again known in advance. This variance tensor $W$ better
  be compact---in some sense---relative to the prior variance $V$, or
  else all the vectors will be considered similar! Interestingly, the
  variance $W$ can be made compact in the relevant ways by having
  small eigenvectors, or by being low-rank, or both. Even more
  interestingly, it can even have very large (infinite, even)
  eigenvectors provided that it is low rank. More on this below.
  If you want the simple case of just measurement noise and a prior,
  then this variance $W$ can be set to precisely zero in what follows.
\end{enumerate}

In the above, I mentioned the word ``low rank''. The matrix $V$ is
low-rank if it can be written as
\begin{eqnarray}
V &=& \tra{R}\cdot\Lambda\cdot R
\label{eq:lowrank}\quad ,
\end{eqnarray}
where $R$ is rectangular, and $\Lambda$ is a diagonal matrix%
\footnote{Actually, $\Lambda$ need not be diagonal, but let's imagine
  that it is, for simplicity, here and later. It is always possible for
  you to \emph{make} it diagonal, by choosing $R$ wisely.}
that is
smaller in dimension than $V$. A place where typical physicists have
encountered low-rank matrices is when they have performed principal
components analysis (\acronym{PCA}) or singular value decomposition
(\acronym{SVD})\footnote{The Wikipedia pages on \acronym{PCA}, \acronym{SVD}, and other
  linear-algebra operations mentioned in this note are all
  excellent.}, taken only the top $K$ eigenvectors, and re-built an
approximation to the variance tensor or matrix that they started with.
When parts of this problem are low-rank, there will be small
linear-algebra tricks that will become available to speed up our
analyses.  The \emph{reason} that these low-rank comments are relevant
here is that the true or prior variance $V$ is almost always low-rank
when the dimension of the problem gets large: For one relevant
example, a stellar spectrum has thousands of pixels, but it's shape is
actually controlled by only a few underlying parameters.  That says
that we ought to keep the concept of low-rank $V$ in mind as we
proceed.

Our model is going to be that the difference between the observed
spectra is composed of a true, interesting component, a true,
uninteresting component, and some measurement noise. We are going to
marginalize out everything except the true, interesting
difference. Our marginalized posterior pdf for this interesting
difference will produce the scalar we seek. In equations, our model is:
\begin{eqnarray}
  y_1 - y_2 &=& \Delta + \delta + \epsilon
\\
  p(\Delta) &=& \normal(\Delta\given 0,V)
\\
  p(\delta) &=& \normal(\delta\given 0,W)
\\
  p(\epsilon) &=& \normal(\epsilon\given 0,C_1+C_2)
\quad ,
\end{eqnarray}
where $\Delta$ is the difference that is interesting and important
(and that will be zero if the vectors are truly identical), $\delta$
is the difference that is uninteresting but real, and $\epsilon$ is
pure measurement noise.

\paragraph{Inference:}
From these, we can construct a marginalized posterior pdf
$p(\Delta\given y_1,y_2)$ for the true, interesting difference
$\Delta$, by multiplying together the pdfs and integrating out
$\epsilon$ and $\delta$. This marginalization produces
\begin{eqnarray}
  p(\Delta\given y_1,y_2) &=& \normal(\Delta\given\mu,\Sigma)
\\
  \Sigma &\equiv& \inv{(\inv{[V]} + \inv{[C_1 + C_2 + W]})}
\\
  \mu &\equiv& \Sigma\cdot\inv{[C_1 + C_2 + W]}\cdot[y_1-y_2]
\quad ,
\end{eqnarray}
where $C$ is a variance matrix, and $\mu$ is an expectation value.
These results were found by making heavy use of the following exact
identity\footnote{For which I owe great thanks to Boris Leistedt
  (NYU), and also my old friend Sam Roweis (deceased), whose notes on
  Gaussian identities and matrix identities I carry with me always. At
  time of writing these notes can be reached at \url{foo} and
  \url{bar}.}
for Gaussian functions:
\begin{eqnarray}
  \normal(x\given a,A)\,\normal(x\given b,B) &\equiv& \normal(a\given b,A+B)\,\normal(x\given d,D)
\\
  D &\equiv& \inv{[\inv{A} + \inv{B}]}
\\
  d &\equiv& D\cdot [\inv{A}\cdot a + \inv{B}\cdot b]
\quad .
\end{eqnarray}

There are many comments to be made here. One is that the matrix $V$
(which appears in our fomula for $\Sigma$ and therefore also $\mu$)
does not necessarily have an inverse!\footnote{The matrix $V$ will not
  have an inverse when it is low rank. For discussion of low rank, see
  above in the vicinity of \equationname~(\ref{eq:lowrank}).} In which
case, inverting it can be avoided by noting the following identity:
\begin{eqnarray}
  \inv{[\inv{A} + \inv{B}]} &\equiv& A\cdot\inv{[A + B]}\cdot B
\quad .
\end{eqnarray}
That is, we do not require that the prior variance $V$ be positive
definite; it only need be non-negative semi-definite.
Another comment to be made is that as $V$ gets full-rank and large
in all directions, the expectation $\mu$ approaches just the difference
$[y_1-y_2]$.
Another comment is that the variance $W$ appears in parallel with the
noise covariances. This is not surprising, because $W$ is just another
source of noise in this context. It is intrinsic noise. Another way
to put it: Uninteresting differences are identical to measurement noise.
And it is also the case that $W$ can be zero or have as many zero
eigenvalues as you like.
Yet another comment is that the expectation $\mu$ is an inverse-variance-weighted
mean of zero and the spectral difference $[y_1-y_2]$. And so on!

Now our suggestion (above) was that the scalar we would use to decide
whether two vectors are identical is the posterior pdf $p(\Delta\given
y_1,y_2)$ for the difference $\Delta$, evaluated at $\Delta=0$, or
perhaps the log of that:
\begin{eqnarray}
  Q &\equiv& -2\,\log p(\Delta=0\given y_1,y_2)
\\
    &=& \tra{\mu}\cdot\inv{\Sigma}\cdot\mu
\label{eq:1}\\
    &=& \tra{[y_1-y_2]}\cdot\inv{[V + C]}\cdot V\cdot\inv{C}\cdot[y_1-y_2]
\label{eq:scalar} \\
  C &\equiv& C_1 + C_2 + W
\quad ,
\end{eqnarray}
where we defined $C$ to be the sum of the noise variances to simplify
our expression, and we have dropped some constants\footnote{The
  constants we have dropped are terms involving log determinants of
  various matrices. We can drop them in this context, because we are
  assuming (see Assumptions above) that we know all four tensors
  $C_1$, $C_2$, $V$, and $W$. If that \emph{isn't} true, and this
  analysis were to be expanded to one in which any or all of those
  variance tensors was to be learned or inferred or marginalized out,
  these log-determinant terms could not be ignored. Just a word to the
  wise.} and used some more matrix-algebra foo to get from
  \equationname~(\ref{eq:1}) to (\ref{eq:scalar}).

This scalar $Q$ of \equationname~(\ref{eq:scalar}) has lots of good properties,
and begs various points of discussion:
First, it is probabilistically justified, in the context of the explicit
assumptions I listed in the Assumptions section, above. That beats most
heuristics.
Second, it reduces to exactly $\chi^2$ of \equationname~(\ref{eq:chisquared}) in
precisely the limit where we would expect it. This limit is that in which the
intrinsic variance $W$ vanishes (so the noise variance is just $C = C_1 + C_2$), and the prior
variance is large in all directions (so $V + C$ approaches $V$).
That is, this whole formalism strongly endorses my original advice to ``just''
use $\chi^2$.
Third, if $V$ is low-rank (as described in and around
\equationname~(\ref{eq:lowrank}); that is, so there is no prior
variance along some directions), we only care about the data
directions that have non-zero prior variance. The multiply by $V$ in
the middle of \equationname~(\ref{eq:scalar}) projects the data
difference down to the non-zero directions in data space, as we want.
Fourth, the operation $V\cdot\inv{C}$ is effectively taking the ratio
of the prior variance to the noise variance. This operation up-weights
directions in data space where the measurements are very good (very
precise) relative to the prior, and down-weights directions where the
noise variance is larger than the prior. After all, these directions
(where the noise is worse than the prior) are directions where the
data we have are not valuable in assessing similarity.\footnote{This
  aspect might project onto things that Andy Gould (OSU) was saying at
  people in Heidelberg. It also answers one of the raw objections to
  plain-old $\chi^2$ that I raised near the beginning.}

So---in sum---we weren't so wrong to be using $\chi^2$ in the first
place, but we \emph{were} able to address the point that some directions
in the data space are far more valuable than others, in general.

The biggest issue with our scalar $Q$ is that---as with $\chi^2$---we
can compute it, but we don't know what to \emph{do} with it. If, for
example, we want to make a list of identical pairs, we have to choose
a threshold, below which we call the pair identical, and above which
we don't. Choosing that threshold, if we want to (say) make a sample
of twins, is necessary, and it is also necessarily heuristic. I have
an infinite number of thoughts about this, way more than I can write
in this \documentname, and also retain my day job. Some people think about
this in terms of false-positive and false-negative rate, which maps
onto the receiver-operator characteristic (\acronym{ROC}) Others think
about this in terms of utility; I fall into this latter
category\footnote{Dustin Lang and I have done so explicitly in the
  astrophysics literature (Lang \etal, 2010).},
but suffice it to say that any threshold-setting necessarily happens
outside the context of probabilistic inference.

Another comment about threshold-setting is that, in the context of
fixed $V$ and $W$, vectors with different quality
measurements---different noise tensors $C_1$ and $C_2$ will be found
to be identical with different false-positive and false-negative rates
(or, maybe, type-1 and type-2 error frequencies\footnote{%
  Different communities have different names for these kinds of errors.
  In astronomy, these are sometimes referred to as contamination and
  incompleteness. I also see other terminologies in the engineering
  world (HOGG INCLUDE PRECISION / PURITY or whatever).}
) at any given
threshold. That is, one threshold perhaps isn't the right tool for all
jobs. Also, in different $V$ and $W$ contexts, a single threshold will
deliver decisions or results with different qualities as $V$ and $W$
change. That is, threshold-setting is not only non-trivial and
necessarily heuristic, but those heuristics might end up delivering
results that are themselves a strong function of $C_1$, $C_2$, $V$,
and $W$. One consequence of all this is that if your data are
heterogenous (that is, there is diversity in the noise variances),
your results will also be heterogeneous. There will be no way, in
general, to keep the false-positive and false-negative rate constant
across any heterogenous sample.

\paragraph{Linear algebra:}
The patient reader may complain that there has already been enough
linear algebra.\footnote{Even though there hasn't been nearly as much
  mathematical mess written in this \documentname\ as there has been written
  in my notebooks getting to this point!}  But there hasn't, because I
have been talking about the point that $V$ might be low rank in the
sense of \equationname~(\ref{eq:lowrank}), and how this lower rank
might help us, or how we might modify the way we compute things if it
\emph{is} low-rank.

The principal and most valuable fact to know about low-rank matrices
is that when something is low-rank, you can make use of the matrix
inversion lemma\footnote{Also called the ``Woodbury Matrix Identity''
  and described usefully here:
  \url{https://en.wikipedia.org/wiki/Woodbury_matrix_identity}. I
  don't use the ``Woodbury'' name for this because I don't like
  calling things after people's names when I don't have to. (One inconsistency:
  I say ``Gaussian'' and not ``normal''. My excuse is: The word ``normal'' is over-loaded!)}
\begin{eqnarray}
\inv{[V + C]} &=& \inv{[\tra{R}\cdot\Lambda\cdot R + C]}
\\
 &\equiv& \inv{C} - \inv{C}\cdot\tra{R}\cdot \inv{[\inv{\Lambda} + R\cdot \inv{C}\cdot\tra{R}]}\cdot R\cdot\inv{C}
\quad ,
\end{eqnarray}
where we have used the low-rank description of \equationname~(\ref{eq:lowrank}).
This might not look like a simplification, but it usually is, speed-wise, and
also often for numerical stability.
For example, in the extremely common case that $C$ is diagonal (all
noise is axis-aligned, or independent from data dimension to data
dimension), and $V$ is very low-rank, this is all very fast to
compute.

Another piece of unsolicited advice: When any matrix $C$ or $\Lambda$
is diagonal, don't instantiate it as a full matrix with almost every
element zero, and don't invert it using the inversion code in your
linear-algebra library: Invert the diagonal and keep it just as a list
of diagonal elements! That's just good code practice.

Yet another piece of unsolicited advice: It is often and usually
better to compute \code{solve(A, y)} than \code{dot(inv(A), y)}. That
is, the \code{solve()} operation is usually more stable than the
matrix inverter.  Also, if you are doing the same solve or inversion
over and over again, you can usually get your matrix library to carry
forward or save a factorization.\footnote{Another piece of advice, but
  much less relevant to the present \documentname, is to use
  \code{logdet()} or \code{slogdet()} but not \code{det()} when you
  need determinants.  Indeed, the \code{det()} operation is probably
  an exponentiation of the \code{slogdet()} operation anyway, and
  subject to unpleasant underflows or overflows.}

\paragraph{Prior and intrinsic scatter:}
While astrophysicists---and other natural scientists---are
very good at delivering estimates
of the observational uncertainty variance tensors $C_1$ and $C_2$, it
is another matter entirely to determine the variance $V$ of the prior
over true spectra, or the variance $W$ of the true differences that
you don't care about or want to ignore.%
\footnote{In many cases, of course, even the uncertainty estimates
  $C_1$ and $C_2$ are poor, or under-estimated, or made under
  unrealistic assumptions. That problem is out of scope, but there are
  also many empirical and sensible ways to estimate or adjust these
  tensors too, given a good set of data. One of the best ways is by
  looking at repeat observations!}
How might we determine%
\footnote{Recall that we must determine these in order to use the
  method and results in this \documentname: It is literally part of
  the assumptions that we have acceptable estimates of $V$ and $W$
  before we start.} these tensors?

A full treatment of how you might obtain and assess prior variance $V$
and nuisance variance $W$ are firmly outside the scope of this
\documentname. However, here are some comments of possible relevance:
First, if your observations are precise, and the population is broad
in most or all dimensions, then just take the limit of large $V$. As
noted above, if you do this, you get $\chi^2$ (though with variance
$C\equiv C_1 + C_2 + W$). This is very often a good strategy when the
data are low-dimensional ($D$ is a few).

Second, if your observations are precise, but you think that the prior
variance $V$ is low-rank, use \acronym{PCA} on a representative data
sample---that is, a representative set of observations. Inspect the
eigenvalues out of \acronym{PCA}, decide on the number $K$ of eigenvalues
you are going to keep, and build
$V$ out of those top $K$ eigenvalues $\lambda_k$ and eigenvectors $e_k$ by
\begin{eqnarray}
  V &=& \sum_{k=1}^K \lambda_k\,e_k\cdot\tra{e_k}
\\
    &\equiv& \tra{R}\,\Lambda\,R
\quad ,
\end{eqnarray}
where $\tra{R}$ is the $D\times K$ rectangular matrix that contains
the $K$ eigenvectors $e_k$, and $\Lambda$ is the $K\times K$ diagonal matrix
with the eigenvalues $\lambda_k$ on the diagonal\footnote{%
  Compare to \equationname~(\ref{eq:lowrank})},
and the reader is invited to recall
that (here) vectors like $e_k$ are always column vectors, so that
$e_k\cdot\tra{e_k}$ is a square matrix outer product.\footnote{We
  are using the matrix notation and conventions I learned from Kusse
  \& Westwig (2006). Well, really I got it from an earlier edition than the 2006 edition!})
This \acronym{PCA} approach
has the advantage that it is super-simple, but the disadvantage
that \acronym{PCA} gives an approximation to the \emph{empirical} variance, which
has both true-scatter and observational-scatter contributions.%
\footnote{I am a big critic of \acronym{PCA}. It describes your
  empirical variance---that is, the variance in your data set coming
  from all sources of variance combined. It doesn't distinguish
  \emph{sources} or \emph{causes} of variance. It involves no
  inference. That makes it easy and simple and fast, but also usually
  wrong.}
But this approach is often
appropriate. In the case of finding stellar stunt doubles (identical
spectra) in the \project{\acronym{APOGEE}} Survey, this is a possibly acceptable
method, simply because the spectra are very high in
signal-to-noise. It is less true in the case of finding chemical
twins, because there the individual-element abundance measurements are
noisy.

Third, if this latter issue is a problem---that is, if the
observational scatter is biasing your $V$ estimate high---you might
want to do some kind of real inference to get $V$. That is, if
components of $C$ are as large as or larger than relevant components
of $V$, you might want to build a model to infer $V$, using as
training data some representative sample of your observations. We have
written up examples of this (Bovy \etal, 2011; Tsalmantza \& Hogg 2012);
one of these examples could be applied to the chemical-twin case.

Fourth, I should comment that estimation of $W$ is more
problematic. Generally you won't have a sensitive scalar $Q$ if $W$ is
large in bad ways. That is, if $W$ has large eigenvalues associated
with eigenvectors that project strongly onto the eigenbasis of $V$,
there won't be much discriminative power (the product $V\cdot\inv{C}$
will become small).%
\footnote{It is left as an exercise to the reader to argue this out in detail.}
So ideally $W$ is low in rank (or, even better,
actually zero). There is no magic
method for finding the low-rank sub-space; it is problem-dependent. In
the case of finding chemical twins, we know that stars born in the
same star cluster have extremely similar chemical abundances, but in
detail there are tiny differences along certain directions in
chemical-abundance space.\footnote{%
  Some small differences are seen in our own work (Ness \etal, 2017), and
  there are known differences that project (tantalizingly) onto the directions
  in abundance space that are expected to be depleted or enhanced by planet
  formation (HOGG CITE SOMETHING??)!}
In this case, empirical
differences among stars that are known to have common origin can be
inspected or \acronym{PCA}-ed or modeled statistically to develop an
estimate for $W$. The \emph{most} ideal case is that both $V$ and $W$
are low-rank, and their bases are orthogonal. Then $W$ doesn't hurt
you at all.

\paragraph{Relaxing the assumptions:}
We began the description of the methods presented in this
\documentname\ with a list of assumptions. Why do we make these
assumptions, and can we relax them?

The principal, over-riding assumption is that everything is Gaussian:
The observational uncertainties, the prior, and the nuisance
(intrinsic scatter). This assumption is required for any fast
method. There are other options that might be nearly as simple to implement,%
\footnote{Search terms like ``L1 regularization'' and ``the Lasso''
  are words that spring to mind. These techniques are used in the
  compressed sensing and convex optimization worlds.}
and there are
definitely other choices that are more accurate in many situations,
but most deviations from Gaussianity lead to either mathematical
intractability, or bad slowness. That said, if the data contain
substantial outliers (large skew or kurtosis), it might be necessary
to take these into account, depending on your goals. It is usually worth
trying the Gaussian case first at least, and then inspecting the
output and issues. Outliers usually appear in obvious ways.

If you must depart from Gaussianity, I would recommend starting with
mixtures of Gaussians. These retain many of the valuable properties of
Gaussians and just elevate some of our linear algebra-expressions up
to large sums of linear-algebra expressions. I used the word ``just''
there, but the derivations might be challenging combinatorically, if
all four variances (two observational, one prior, and one nuisance)
all elevate to Gaussian mixtures.\footnote{Because our scalar $Q$ is
  related to the logarithm of a probability, the new scalar that would
  appear in the mixture-of-Gaussians case would be a \code{logsumexp()}
  of $Q$-like terms, with weights.}

In many cases (like for instance in Oh \etal, 2017), there are
additional non-linear parameters that affect either the data, or the
implied difference, or the variance tensors. That is, there are some
additional search parameters that have to be either optimized or
marginalized out or grid-searched. This brings in a substantial change
if there is any sense in which there are parameters that affect the
tensors $C_1$, $C_2$, $V$, and $W$: If we vary these tensors, we must
also vary some log-determinant terms that I have deliberately
dropped. If you go there, you will have to re-derive all expressions,
maintaining those terms.

In some sense the worst assumption in this document is that the four
tensors $C_1$, $C_2$, $V$, and $W$ are all presumed to be known well.
That is almost never true in any investigation I have been involved
in.  If you want to infer these or marginalize them out, however, you
need to build a hierarchical noise and population model on top of
everything I have written here. You will need your log-determinant
terms, and also priors over new parameters. This gets into the full
space of hierarchical graphical models, which is way beyond the
current scope. That said, there are lots of great packages and growing
community knowledge about such approaches. For problems like these,
hierarchical models are the long-term solution.

\newcommand{\arXiv}[1]{\href{http://arxiv.org/abs/#1}{\textsl{arXiv}:#1}}
\paragraph{Bibliography}
\begin{list}{}{\itemsep=0pt \parskip=0pt \parsep=0pt \leftmargin=\parindent \itemindent=-\parindent}
\item Bovy,~J., Hogg,~D.~W., \& Roweis,~S.~T., 2011,
  \project{Extreme Deconvolution}: Inferring complete distribution functions from noisy, heterogeneous and incomplete observations,
  \textit{Ann. Appl. Stat.} \textbf{5}(2B), 1657--1677 (\arXiv{0905.2979}).
\item Foreman-Mackey,~D., Hogg,~D.~W., \& Morton,~T.~D., 2014,
  Exoplanet population inference and the abundance of earth analogs from noisy, incomplete catalogs,
  \textit{Astrophys. J.} \textbf{795}(1), 64 (\arXiv{1406.3020}).
\item \project{Gaia} Collaboration, Brown,~A.~G.~A., \etal, 2016,
  \project{Gaia} Data Release 1. Summary of the astrometric, photometric, and survey properties,
  \textit{Astron. Astrophys.} \textbf{595}, A2 (\arXiv{1609.04172}).
\item Kusse,~B.~R. \& Westwig,~E.~A., 2006,
  \textit{Mathematical Physics: Applied Mathematics for Scientists and Engineers},
  2ed, Wiley (\acronym{ISBN}: 978-3-527-40672-2)
\item Lang,~D., Hogg,~D.~W., Mierle,~K., Blanton,~M., \& Roweis,~S., 2010,
  \project{Astrometry.net}: Blind astrometric calibration of arbitrary astronomical images,
  \textit{Astron. J.} \textbf{139}(5) 1782--1800 (\arXiv{0910.2233}).
\item Majewski,~S.~R., \etal, 2015,
  The \project{Apache Point Observatory Galactic Evolution Experiment} (\project{\acronym{APOGEE}}),
  \arXiv{1509.05420}.
\item Ness,~M., \etal, 2017,
  Galactic Doppelganger: The chemical similarity among field stars and among stars with a common birth origin,
  \arXiv{1701.07829}.
\item Oh,~S., Price-Whelan,~A.~M., Hogg,~D.~W., Morton,~T.~D., Spergel,~D.~N., 2016,
  Co-moving stars in \project{Gaia DR1}: An abundance of very wide separation co-moving pairs,
  \arXiv{1612.02440}.
\item Tsalmantza,~P. \& Hogg,~D.~W., 2012,
  A data-driven model for spectra: Finding double redshifts in the \project{Sloan Digital Sky Survey},
  \textit{Astrophys. J.} \textbf{753}(2) 122 (\arXiv{1201.3370}).
\end{list}

\end{document}
